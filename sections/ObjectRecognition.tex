\section{Object Recognition\buch{Ch. 12}}
\begin{itemize}
\item Recognition of \emph{individual} image regions called \emph{object} or \emph{pattern}
\item Concept of ``learning'' from sample patterns
\end{itemize}

\subsection{Patterns and Pattern Classes\buch{Ch. 12.1}}
\begin{itemize}
	\item A \emph{pattern} is an arrangement of descriptors
		(as in \ref{sec:representationDescription}) in pattern classification
		called \emph{features}.
	\item A \emph{pattern class} is a set of patterns, denoted $\omega_1, \omega_2, ..., \omega_W$ W is the number of classes
	\item The goal is to assign a given patter into a class
\end{itemize}

\subsubsection{Vectors}
\begin{itemize}
	\item Used for quantitative descriptors
	\item every dimension contains the numerical value of a descriptor
	\item $\mathbf{x} =
		\begin{bmatrix}
			x_1 \\
			x_2 \\
			\vdots \\
			x_n \\
		\end{bmatrix}
		 = (x_1, x_2, \ldots, x_n)^T $
\end{itemize}
Example: Classifying flowers based on petal length and width into one of three
classes.

\subsubsection{Strings}
Structural information can be captured in strings. Example
\nameref{sec:boundaryDescriptors}
\begin{itemize}
	\item Connectivity patterns (order is important)
	\item Compact description, better than sampling into a feature vector
\end{itemize}

\subsubsection{Tree}
If there is hierarchy then a descriptor which is based on a tree structure
should be used.

% TODO: Tikz Tree einfÃ¼gen

\subsection{Recognition Based on Decision Theory\buch{Ch. 12.2}}
\paragraph{Discriminant functions}
The core of decision theory $d_i(\mathbf{x})$ defined for each class $\omega_i$.
These are not known and finding them is the main goal. \\

A class $\omega_i$ is selected if
\begin{align*}
d_i(\mathbf{x}) > d_j(\mathbf{x}) && j=1,2, \cdots, W; j \neq i
\end{align*}
The interesting parts are decision boundaries between classes
\begin{align*}
	d_i(\mathbf{x}) - d_j(\mathbf{x}) = 0
\end{align*}
For two classes, a single discriminant function can be introduced
\begin{align*}
	d_{ij}(\mathbf{x}) = d_i(\mathbf{x}) - d_j(\mathbf{x}) = 0
\end{align*}

\subsubsection{Matching\buchSeite{866}}
Each class is represented by prototypes.
A vector $x$ belongs to the class with the shortest distance.
% TODO: Tikz Diagramm mit klassen, prototypen und grenzen
\paragraph{Minimum distance classifier}
The mean vector of a class is a powerful prototype vector.
\begin{align*}
	\mathbf{m}_j = \frac{1}{N_j} \sum_{\mathbf{x} \in \omega_j}\mathbf{x}_j && j = 1,2,\dots, W
\end{align*}
A simple matching function is the Euclidean distance ($||a|| = (a^Ta)^{1/2}$)).
\begin{align*}
	D_j(\mathbf{x}) = ||\mathbf{x}-\mathbf{m}_j|| = \left((\mathbf{x}-\mathbf{m}_j)^T(\mathbf{x}-\mathbf{m}_j)\right)^{1/2}
\end{align*}
The resulting discriminant function which is the largest, when the distance is
the smallest.
\begin{align*}
	d_j(\mathbf{x}) = \mathbf{x}^T \mathbf{m}_j - \frac{1}{2}\mathbf{m}_j^T \mathbf{m}_j
\end{align*}
The decision boundary between class $\omega_i$ and $\omega_j$ will be
\begin{align*}
	d_{ij}(\mathbf{x}) &= d_i(\mathbf{x}) - d_j(\mathbf{x})\\
	&= \mathbf{x}^T(\mathbf{m}_i-\mathbf{m}_j) - \frac{1}{2}(\mathbf{m}_i-\mathbf{m}_j)^T(\mathbf{m}_i+\mathbf{m}_j) = 0
\end{align*}
Which is a line through the midpoint of $m_i$ and $m_j$ for $n=2$.
For $n=3$ its a plane and for $n>3$ it's a hyperplane.
% TODO: Tikz mit diagram
% TODO: Evt. Beispiel?

\paragraph{Matching by correlation\buchSeite{869}}
Calculate cross-correlation between a mask $w(x,y)$ of size $m\times n$ and an image
$f(x,y)$ to find the mask in the image.
\begin{align*}
	c(x,y) = \sum_s\sum_tw(s,t)f(x+s,y+t)
\end{align*}
This is sensitive to contrast and average intensity in $f$ and $w$.
Hence the normalized cross correlation is used.
Note $\bar{f}_{xy}$ is the average value of $f$ in the region $w$.
\begin{align*}
	\gamma(x,y) = \frac{\sum_s\sum_t\left[w(s,t)-\bar{w}\right]\left[f(x+s,y+t)-\bar f_{xy}\right]}
	{
	\left\lbrace
		\sum_s\sum_t\left[w(s,t)-\bar w\right]^2
		\sum_s\sum_t\left[f(x+s,y+t)-\bar f_{xy}\right]^2
	\right\rbrace^{\frac{1}{2}}
	}
\end{align*}
$\gamma(x,y)$ is always in range $[-1, 1]$, where +1 is perfect match and -1 a perfect anti-match.
This is sensitive to scale and rotation.
Often the square root is ignored and absolute values are used instead of squares to save computational effort.

\subsubsection{Optimum Statistical Classifiers\buchSeite{872}}
A classifier which results in the lowest average risk of incorrect classification. \\

The probability that a pattern $\mathbf{x}$ comes from class $\omega_i$ is the conditional probability $p(\omega_i | \mathbf{x})$.
The loss that occurs if a pattern $\mathbf{x}$ is put to class $\omega_j$ instead of $\omega_i$ is denoted as $L_{ij}$. \\

The conditional average risk or loss is the average loss if $\mathbf{x}$ is assigned to class $j$:
	\[
		r_j(\mathbf{x}) = \sum\limits_{k=1}^{W} L_{kj} p(\omega | \mathbf{x}) = \frac{1}{p(\mathbf{x})} \sum\limits_{k=1}^{W} L_{kj} p(\mathbf{x} | \omega_k) P(\omega_k)
	\]
As $p(\mathbf{x})$ is equal to all $r_j$ and assuming that the loss for every misclassification is equal (e.g. 1) this is simplified to
	\[
		r_j(\mathbf{x}) = p(\mathbf{x}) - p(\mathbf{x} | \omega_j) P(\omega_j)
	\]
Or written in the same form as before
	\[
		d_j(\mathbf{x}) = p(\mathbf{x} | \omega_j) P(\omega_j)	\qquad j=1,2,\ldots,W
	\]
In order to estimate $p(\mathbf{x}|\omega_j)$ and $P(\omega_j)$, one uses a Gaussian assumption. 
If $N_j$ is the number of pattern vectors from class $\omega_j$, the mean vector $\mathbf{m}_j$ and the covariance matrix $\mathbf{C}_j$ are calculated as
\begin{align*}
	\mathbf{m}_j = \frac{1}{N_j} \sum\limits_{\mathbf{x}\in\omega_j} \mathbf{x}
	&&
	\mathbf{C}_j = \frac{1}{N_j} \sum\limits_{\mathbf{x}\in\omega_j} \mathbf{x}\mathbf{x}^T - \mathbf{m}_j\mathbf{m}_j^T
\end{align*}
The decision function can then be written as
	\[
		d_j(\mathbf{x}) = \ln P(\omega_j) - \frac{1}{2}\ln|\mathbf{C}_j| - \frac{1}{2}\left[(\mathbf{x}-\mathbf{m}_j)^T\mathbf{C}_j^{-1}(\mathbf{x}-\mathbf{m}_j)\right]
	\]

\subsubsection{Neural Networks\buchSeite{882}}

Class of algorithms which find decision functions directly by training. Neural Netowrks (NN) make no Gaussian assumption.

\paragraph{Perceptron for two pattern classes}

The perceptron learns a linear decision function that separates two linearly separable training sets.

\begin{figure}[htb]
\centering
\adjustbox{scale=1.2}{\input{tikz/objectrecognition/perzeptron}}
\caption{Perceptron model for two pattern classes}
\label{fig:perceptron}
\end{figure}
\begin{align*}
d(\mathbf{x}) = \sum_{i=1}^nw_ix_i + w_{n+1}
\end{align*}
If $d(\mathbf{x}) > 0$, then the pattern belongs to $\omega_1$, if  $d(\mathbf{x}) < 0)$ it belongs to $\omega_2$. \\

The function which maps the weighted sum of the input to the output is called \emph{activation function}.
In \ref{fig:perceptron} a sign function is used.
For multilayer NN a sigmoid (\ref{fig:sigmoid}) is common. \\

The constant input of 1 allows an offset.
A convenient notation is the augmented pattern vector, which allows to handle the offset as if it was another input

\begin{align*}
y_i &= x_i & i=1,2,\dots,n \\
y_{n+1} &= 1 \\
d(y) &= \sum_{i=1}^{n+1}w_iy_i = w^Ty \\
\end{align*}

\paragraph{Training algorithms\buchSeite{886}}
Train the weights of the perceptron so that it can distinguish between classed, using training sets.
For linear separable classes, a simple, iterative algorithm is:
\begin{itemize}
\item
Pattern vectors of the training set are presented repetitively, the weight vector is changed if a misclassification happens.
\item The training stops, when there are no more misclassifications.
\end{itemize}

for $d(y)>0 \rightarrow \omega_1$ and $d(y)<0 \rightarrow \omega_2$

\begin{align*}
\text{if } y(k) \in \omega_1 \text{ and } w^T(k)y(k) \leq 0 && \rightarrow && w(k+1) = w(k) +cy(k) \\
\text{if } y(k) \in \omega_2 \text{ and } w^T(k)y(k) \geq 0 && \rightarrow && w(k+1) = w(k)-cy(k)
\end{align*}


$c$ is a positive correction increment.
This algorithm converges in a finite number of steps for linearly separable training data.

This is not possible for nonseparable classes.
Hence minimizing the squared error is the target.

\begin{align*}
J(w) = \frac{1}{2}(r-w^Ty)^2
&& r \text{ is the desired response and $y$ the pattern vector}
\end{align*}

To minimize this we use the delta correction algorithm
\begin{align*}
\Delta w &= \alpha e(k)y(k) \\
\Delta w &= w(k+1)-w(k) && \text{weighting change} \\
e(k) &= r(k) - w^T(k)y(k) && \text{error}
\end{align*}

The change of the error becomes
\begin{align*}
\Delta e &= -\alpha e(k) y^T(k) y(k) \\
 &= - \alpha e(k) ||y(k)||^2
\end{align*}

So $\alpha$ controls the speed of error reduction.
For stability $\alpha$ must be smaller than 2.
In practice $\alpha$ is picked below 1.

\paragraph{Multilayer feedforward neural networks\buchSeite{889}}
These allow for multiclass pattern recognition, also for non linear separable classes.
They are stacked banks of perceptrons with the sigmoid as activation function (\ref{fig:sigmoid}).

\begin{figure}[htp]
\centering
\input{tikz/objectrecognition/nn}
\caption{Multilayer feed forward network}
\end{figure}

\begin{figure}[htp]
\centering
\input{tikz/objectrecognition/sigmoid}
\caption{Sigmoid function}
\label{fig:sigmoid}
\end{figure}

If $K$ is the layer preceding layer $J$ then the input to the activation function of node $j$ in layer $J$ is the weighted sum of the outputs of Layer $K$.

\begin{align*}
I_j = \sum_{k=1}^{N_k}w_{jk}O_k
\end{align*}

\paragraph{Training by back propagation}
Target is to minimize the total squared error between the desired responses
$r_q$ and the actual response $O_q$ in the output layer $Q$
\begin{align*}
	E_Q = \frac{1}{2} \sum_{q=1}^{N_Q}(r_q-O_q)^2
\end{align*}

Back-Propagation is deriving the Error with respect to the weights
\begin{align*}
	\Delta w_{qp} &= -\alpha \frac{\partial E_Q}{\partial w_{qp}}
\end{align*}
$\Delta w$ is the change in weight vector, $\alpha$ the correction increment,
$E_Q$ the error and $w_{qp}$ the current weight vector

\begin{itemize}
\item A training vector is presented at
the input and the desired response
at the output
\item For all nodes, the input and output values are calculated
\item Starting at the output layer, the
deltas are calculated and back
propagated
\item Now the weights are updated
\end{itemize}


\begin{align*}
\Delta w_{jk} &= \alpha \delta_j O_k& \\
\delta_j &= (r_j-O_j) h'_j(I_j) & \text{if layer $J$ is the output Layer} \\
\delta_j &= h'_j(I_j) \sum_{p=1}^{N_p}\delta_p w_{jp} & \text{if layer $J$ is an internal layer and layer $P$ is the next layer}
\end{align*}

Since $sig'(t) = sig(t)(1-sig(t))$ for $\theta_0=1$ the derivative of the activation function becomes
\begin{align*}
h'_j(I_j) = O_j(1-O_j)
\end{align*}

\paragraph{Some facts about NN}
\begin{itemize}
\item Training with noisy data helps improving performance
\item Training with more patterns also
\item Single layer perceptron results in hyperplanes
\item Two layers result in open or closed convex regions
\item Three layers result in arbitrary (limited by the number of nodes) regions
\item More than three layers are never needed
\end{itemize}

\subsection{Structural Methods\buch{Ch. 12.3}}

\subsubsection{Shape numbers}
Remember that the order $n$ of a shape number is the number of digits. This is clearly even for closed boundaries. \\

To match shape numbers, the degree of similarity $k$ between two shapes $a$ and $b$ is introduced. 
It is defined as the largest order $j$, for which the shape numbers $s_j$ still coincide.
\begin{align*}
	s_j(a) = s_j(b) \qquad & \text{for} \quad j=4,6,8,\ldots,k \\
	s_j(a) \neq s_j(b) \qquad & \text{for} \quad j=k+2,k+4,\ldots
\end{align*}
The distance between shape $a$ and $b$ is now
	\[
		D(a,b) = \frac{1}{k}
	\]
From this definition follows that
\begin{align*}
	D(a,b) \geq 0 && D(a,b) = 0 \quad \text{if} \quad a=b && D(a,c) \leq \max[D(a,b),D(b,c)]
\end{align*}

\subsubsection{String Matching}
Assume that two shapes $a,b$ are coded into strings $a_1a_2\ldots a_n$ and $b_1b_2\ldots b_n$. 
Let $\alpha$ be the number of matches where $a_k = b_k$. 
The number of symbols that do not match is $\beta = \max[|a|,|b|] - \alpha$ \\

A simple measure of similarity between $a$ and $b$ is now
	\[
		R = \frac{\alpha}{\beta} = \frac{\alpha}{\max[|a|,|b|] - \alpha}
	\]
$R$ is $0$ for no match at all, and is $\infty$ for the perfect match. 
Of course it is important that the strings are aligned. 
Otherwise, one string must be shifted circularly and $R$ is calculated at each position.
The largest $R$ now represents the best match.